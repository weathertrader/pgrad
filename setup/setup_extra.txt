

hostname –I
curl -4 icanhazip.com

100.115.92.200

curl -i http://100.115.92.200
curl -i http://100.115.92.200/todo

curl -i -H "Content-Type: application/json" -X POST -d '{"todo": "Dockerize Flask application with MongoDB backend"}' http://100.115.92.200/todo

http://100.115.92.200/
http://localhost:8888/
http://localhost:8888/
http://0.0.0.0:8888/


24.6.126.210

172.17.0.1 
172.18.0.1 
172.19.0.1 
172.20.0.1 
 

airflow - just -finished
mongodb - just finished
flask apis - pretty far along already
Docker - developing locally and deploying to AWS 
Go / C++ / Scala / Kubernetes / Serverless ETLs

24.6.126.210/32

http://35.164.84.231/pgrad.html
http://35.164.84.231:8080/admin/

ec2-35-164-84-231.us-west-2.compute.amazonaws.com/pgrad.html

index.html access 

ssh -i ~/.ssh/sundownerwatch-IAM-keypair.pem ubuntu@ec2-35-164-84-231.us-west-2.compute.amazonaws.com


scp -i ~/.ssh/sundownerwatch-IAM-keypair.pem ubuntu@ec2-35-164-84-231.us-west-2.compute.amazonaws.com:~/pgrad/images/archive/del_slp_all_model_KWMC-KSFO_* .

######################################
# airflow 


This unit file needs a user called airflow, 
but if you want to use it for a different user, 
change the directives User= and Group= to the desired user. 
You might notice that the EnvironmentFile= and ExecStart= directives are changed. 
The EnvironmentFile= directive specifies the path to a file with environment 
variables that can be used by the service. Here you can define variables like 
SCHEDULER_RUNS, AIRFLOW_HOME or AIRFLOW_CONFIG. Ma


sudo apt-get install postgresql postgresql-contrib libpq-dev python3-psycopg2

pip install apache-airflow[postgres]
pip install psycopg2

airflow.cfg
change executor to 
executor = LocalExecutor
# sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@localhost/airflow
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@localhost:5432/airflow

# maybe change
parallelism = 2
dag_concurrency = 2
max_active_runs_per_dag = 2
#load_default_connections = True
load_default_connections = False

# create the airflow tables in postgres
sudo -u postgres psql
CREATE USER airflow WITH PASSWORD 'airflow';
CREATE DATABASE airflow;
# GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO airflow;
# GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO airflow;
GRANT ALL PRIVILEGES ON DATABASE airflow to airflow;

# and edit the postgres config files 
sudo vi /etc/postgresql/11/main/pg_hba.conf
didn't change anything
change this 
# IPv4 local connections:
host    all             all             127.0.0.1/32         md5
to this
# IPv4 local connections:
host    all             all             0.0.0.0/0            trust
sudo vi /etc/postgresql/11/main/postgresql.conf
and change 
listen_addresses = 'localhost'
to
listen_addresses = '*'
# restart the pg service 
sudo service postgresql restart

# initialize the airflow db 
airflow initdb 
# copy over the airflow service files 

sudo cp setup/airflow-*.service /etc/systemd/system/.



sudo systemctl enable airflow-webserver
sudo systemctl start airflow-webserver
sudo systemctl enable airflow-scheduler
sudo systemctl start airflow-scheduler
sudo systemctl status airflow-webserver
sudo systemctl status airflow-scheduler

sudo journalctl -u airflow-webserver.service -f
sudo journalctl -u airflow-webserver.service -u airflow-scheduler.service -n 10
sudo journalctl -u airflow-webserver.service -n 10
sudo journalctl -u airflow-scheduler.service -n 10


sudo systemctl daemon-reload
sudo systemctl restart airflow-webserver
sudo systemctl restart airflow-scheduler
sudo systemctl stop airflow-webserver
sudo systemctl stop airflow-scheduler
sudo systemctl disable airflow-webserver
sudo systemctl disable airflow-scheduler

chmod 664 /etc/systemd/system/airflow-scheduler.service

sudo systemctl start airflow-webserver.service
sudo systemctl start airflow-scheduler.service
sudo systemctl status airflow-webserver.service
sudo systemctl status airflow-scheduler.service
sudo systemctl enable airflow-webserver.service
sudo systemctl enable airflow-scheduler.service

# airflow 
######################################

-yqq answers yes to prompts and supresses output

sudo apt-get -yqq update

#############################################
# install docker on debian 10 chromebook
sudo apt install apt-transport-https ca-certificates curl gnupg2 software-properties-common
curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable"
sudo apt update
sudo apt install docker-ce docker-compose
sudo systemctl status docker
sudo usermod -aG docker csmith

#############################################
# install docker on ubuntu 20.04 
sudo apt update
sudo apt install apt-transport-https ca-certificates curl software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable"
sudo apt update
sudo apt install docker-ce docker-compose
sudo systemctl status docker
sudo usermod -aG docker ubuntu 


######################################
# docker 

# build a Dockerfile and label the image
docker build -t weathertrader1/flask-app .


# list images
docker images

# run the image and give it a name 
# -p host_port:container_port
# --rm removes container after running 
# --name your_name name the container 
docker run -p 8888:5000 weathertrader1/flask-app
# run as a daemon
docker run -d -p 8888:5000 --name name-flask-app weathertrader1/flask-app
# run in interactive mode
docker run -it weathertrader1/flask-app bash
# attach to an already running container 
docker exec -it weathertrader1/flask-app bash
# run interactively and remove after 
docker run -it --rm weathertrader1/foodtrucks-web bash

# list running images 
docker ps

# list all images 
docker ps -a 

# stop a container 
docker stop es foodtrucks-web
docker stop id

# remove a container 
docker rm es foodtrucks-web
docker rm id

# commit image to repository 
docker commit -m "What you did to the image" -a "Author Name" container_id repository/new_image_name
docker commit -m "added Node.js" -a "sammy" d42d0bbfbd35 sammy/ubuntu-nodejs

# pull image from repository
docker pull image_name

# tag an image
docker tag local-image:tagname weathertrader1:tagname

# push image to repository
docker push weathertrader1:tagname
docker push weathertrader1/foodtrucks-web

# compose 
# docker-compose up does build and run in one step 
# -d daemonize the process 
docker-compose build
docker-compose up
docker-compose up -d
docker-compose -p
docker-compose down -v
# get environmental variables available to container 
docker-compose run web env
docker-compose stop
docker-compose down --volumes
# list images in yml file 
docker-compose ps

db.createUser({user: 'flaskuser', pwd: 'your_mongodb_password', roles: [{role: 'readWrite', db: 'flaskdb'}]})


mongo -u flaskuser -p your_mongodb_password --authenticationDatabase flaskdb


# network 
docker network ls
docker network inspect bridge
docker network create foodtrucks-net
docker network rm foodtrucks-net
docker run -it --rm --net foodtrucks-net weathertrader1/foodtrucks-web bash
docker run -d --name es --net foodtrucks-net -p 9200:9200 -p 9300:9300 -e "discovery.type=single-node" docker.elastic.co/elasticsearch/elasticsearch:6.3.2
docker run -d --net foodtrucks-net -p 5000:5000 --name foodtrucks-web weathertrader1/foodtrucks-web

# mount a volume 
docker run -d -p 8080:8080 -v /path/to/dags/on/your/local/machine/:/usr/local/airflow/dags  puckel/docker-airflow webserver
docker run -d -p 8080:8080 -v <local_path_to_your_dags>:/usr/local/airflow/dags  puckel/docker-airflow webserver
docker run ... -v /var/www/docker:/var/www ... <Your image name>
{host} docker run -v /path/to/hostdir:/mnt --name my_container my_image
{host} docker exec -it my_container bash
{container} cp /mnt/sourcefile /path/to/destfile
docker cp /home/ubuntu/docker-work/sample.html apache:/usr/local/apache2/htdocs/
https://www.digitalocean.com/community/tutorials/how-to-share-data-between-the-docker-container-and-the-host
https://www.digitalocean.com/community/tutorials/how-to-share-data-between-docker-containers

#####################################
#####################################


sudo apt-get install \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg-agent \
    software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository \
   "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"
   

sudo apt-get install docker



sudo apt-get install \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg-agent \
    software-properties-common


sudo service docker start
docker info 
docker run -d -p 80:8080 chandupriya/nodedemo:0.1
sudo service docker start
sudo usermod -a -G docker ec2-user

touch Dockerfile
FROM ubuntu:18.04

# Install dependencies
RUN apt-get update && \
 apt-get -y install apache2

# Install apache and write hello world message
RUN echo 'Hello World!' > /var/www/html/index.html

# Configure apache
RUN echo '. /etc/apache2/envvars' > /root/run_apache.sh && \
 echo 'mkdir -p /var/run/apache2' >> /root/run_apache.sh && \
 echo 'mkdir -p /var/lock/apache2' >> /root/run_apache.sh && \ 
 echo '/usr/sbin/apache2 -D FOREGROUND' >> /root/run_apache.sh && \ 
 chmod 755 /root/run_apache.sh

EXPOSE 80

CMD /root/run_apache.sh

docker build -t hello-world .
docker run -t -i -p 80:80 hello-world

# run and enter the container
docker run -ti ubuntu
exit 

# show docker processes 
docker ps
docker ps -a 

# restart a container 
docker start -ai 11cc47339ee1

# docker 
######################################




pip install flask_pymongo flask-cors

sudo systemctl start <service-name>
$ sudo systemctl stop <service-name>
$ sudo systemctl restart <service-name>
$ sudo systemctl status <service-name>

/etc/systemd/system/microblog.service
[Unit]
Description=Microblog web application
After=network.target

[Service]
User=ubuntu
WorkingDirectory=/home/ubuntu/microblog
ExecStart=/home/ubuntu/microblog/venv/bin/gunicorn -b localhost:8000 -w 4 microblog:app
Restart=always

[Install]
WantedBy=multi-user.target

# to move to setup 
######################################



# did not work
psycopg2 
# works on ubuntu, does not work on chromebook 
conda install -c conda-forge eccodes cfgrib
# works on chromebook 
pip install cfgrib

conda update conda 
conda config --add channels conda-forge
# does not work
conda install -c conda-forge psycopg2 numpy pandas dask netCDF4 xarray spyder matplotlib
# works 
conda install -c conda-forge psycopg2 numpy pandas dask xarray matplotlib
# works on ubuntu, does not work on chromebook 
conda install -c conda-forge eccodes cfgrib
# works on chromebook 
sudo apt-get install libeccodes0
pip install cfgrib
python -m cfgrib selfcheck
cd pgrad
# pip install -r requirements.txt

conda create -n cartopy -c conda-forge cartopy netCDF4 xarray 
conda activate cartopy
conda install spyder‑kernels


create test files on s3://to_process
temp1.txt
temp2.txt
etc

import boto3

s3 = boto3.resource('s3')

def upload_file_to_S3(filename, key, bucket_name):
    s3.Bucket(bucket_name).upload_file(filename, key)

# General ssh and scp commands 

# ssh
ssh -i ~/.ssh/sundownerwatch-IAM-keypair.pem ubuntu@ec2-54-202-214-49.us-west-2.compute.amazonaws.com

# scp
scp -i ~/.ssh/sundownerwatch-IAM-keypair.pem src/* ubuntu@ec2-54-202-214-49.us-west-2.compute.amazonaws.com:/home/ubuntu/raceCast/src/.


# set up a keypair from local to pg
ssh -i ~/.ssh/sundownerwatch-IAM-keypair.pem ubuntu@ec2-34-222-54-126.us-west-2.compute.amazonaws.com


```
Sid not install python-dev but may be needed
sudo apt-get install python3-dev

sudo apt-get install pip
pip3 install psycopg2
pip3 install psycopg2-binary

# create conda environment for pg
conda create -n pg_env
conda activate pg_env
conda install -c conda-forge psycopg2 numpy pandas
pip install spyder‑kernels

sudo apt-get install libpq-dev
# did not install python-dev but may be needed
sudo apt-get install python3-dev
sudo apt-get install python3-psycopg2

sudo apt-get install pip
pip3 install psycopg2
# dont know if this is needed or not
pip3 install psycopg2-binary

# create conda environment for pg
conda create -n pg_env
conda activate pg_env
conda install -c conda-forge psycopg2 numpy pandas
pip install spyder‑kernels

conda install -c conda-forge s3fs

sudo apt-get install s3fs

dash environment 

conda create -n dash_env
conda activate dash_env
conda install -c conda-forge psycopg2 numpy pandas dash falcon
pip install spyder‑kernels

  
df = pd.read_csv('s3://gps-data-processed/gps_stream_3.csv')
fs = s3fs.S3FileSystem(anon=False)
fs = s3fs.S3FileSystem(anon=True)
fs.ls('gps-data-processed')
fs.touch('gps-data-processed/test.txt') 
fs.put(file_path,s3_bucket + "/" + rel_path)
fs = s3fs.S3FileSystem(anon=False, key='<Access Key>', secret='<Secret Key>')


