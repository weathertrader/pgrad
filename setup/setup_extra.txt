
24.6.126.210/32

http://54.70.6.52/

index.html access 
http://54.70.6.52/
http://ec2-54-70-6-52.us-west-2.compute.amazonaws.com/
pgrad.html access
http://ec2-54-70-6-52.us-west-2.compute.amazonaws.com//pgrad.html
http://54.70.6.52/pgrad.html

ssh -i ~/.ssh/sundownerwatch-IAM-keypair.pem ubuntu@ec2-54-70-6-52.us-west-2.compute.amazonaws.com

pip freeze > requirements.txt

conda create -n cartopy -c conda-forge cartopy netCDF4 xarray 
conda activate cartopy
conda install spyder‑kernels


######################################
# airflow 

sudo apt-get update
sudo apt-get install build-essential



in bashrc 
export AIRFLOW_HOME=~/pgrad/airflow

pip install apache-airflow

pip freeze > requirements.txt
pip install requirements.txt

airflow initdb
airflow resetdb
airflow webserver -p 8080
airflow scheduler

airflow scheduler -D
airflow webserver -D

Then you can visit http://localhost:8080 in the browser

Edit airflow.cfg and set 
load_examples = False
catchup_by_default = False


to kill the airflow server 
more ~/pgrad/airflow/airflow-webserver.pid


cat $AIRFLOW_HOME/airflow-webserver.pid | xargs kill -9

ps -aux | grep airflow
kill -9 pid




airflow run example_bash_operator runme_0 2017-07-01

dag goes in 
~/airflow/dag/<your dag>.py


python ~/airflow/dags/tutorial.py

# list active dags 
airflow list_dags
# lists tasks associated with pgrad_dag 
airflow list_tasks pgrad_dag
airflow list_tasks pgrad_dag --tree

airflow test pgrad_dag id_cleanup 2015-06-01


# testing print_date
airflow test tutorial print_date 2015-06-01

# testing sleep
airflow test tutorial sleep 2015-06-01

airflow backfill tutorial -s 2015-06-01 -e 2015-06-07


# airflow 
######################################


create test files on s3://to_process
temp1.txt
temp2.txt
etc

import boto3

s3 = boto3.resource('s3')

def upload_file_to_S3(filename, key, bucket_name):
    s3.Bucket(bucket_name).upload_file(filename, key)

# General ssh and scp commands 

# ssh
ssh -i ~/.ssh/sundownerwatch-IAM-keypair.pem ubuntu@ec2-54-202-214-49.us-west-2.compute.amazonaws.com

# scp
scp -i ~/.ssh/sundownerwatch-IAM-keypair.pem src/* ubuntu@ec2-54-202-214-49.us-west-2.compute.amazonaws.com:/home/ubuntu/raceCast/src/.


# set up a keypair from local to pg
ssh -i ~/.ssh/sundownerwatch-IAM-keypair.pem ubuntu@ec2-34-222-54-126.us-west-2.compute.amazonaws.com


```
Sid not install python-dev but may be needed
sudo apt-get install python3-dev

sudo apt-get install pip
pip3 install psycopg2
pip3 install psycopg2-binary

# create conda environment for pg
conda create -n pg_env
conda activate pg_env
conda install -c conda-forge psycopg2 numpy pandas
pip install spyder‑kernels

sudo apt-get install libpq-dev
# did not install python-dev but may be needed
sudo apt-get install python3-dev
sudo apt-get install python3-psycopg2

sudo apt-get install pip
pip3 install psycopg2
# dont know if this is needed or not
pip3 install psycopg2-binary

# create conda environment for pg
conda create -n pg_env
conda activate pg_env
conda install -c conda-forge psycopg2 numpy pandas
pip install spyder‑kernels

conda install -c conda-forge s3fs

sudo apt-get install s3fs

dash environment 

conda create -n dash_env
conda activate dash_env
conda install -c conda-forge psycopg2 numpy pandas dash falcon
pip install spyder‑kernels

  
df = pd.read_csv('s3://gps-data-processed/gps_stream_3.csv')
fs = s3fs.S3FileSystem(anon=False)
fs = s3fs.S3FileSystem(anon=True)
fs.ls('gps-data-processed')
fs.touch('gps-data-processed/test.txt') 
fs.put(file_path,s3_bucket + "/" + rel_path)
fs = s3fs.S3FileSystem(anon=False, key='<Access Key>', secret='<Secret Key>')


