
24.6.126.210/32

http://35.164.84.231/pgrad.html
http://35.164.84.231:8080/admin/

index.html access 

ssh -i ~/.ssh/sundownerwatch-IAM-keypair.pem ubuntu@ec2-35-164-84-231.us-west-2.compute.amazonaws.com

######################################
# airflow 


This unit file needs a user called airflow, 
but if you want to use it for a different user, 
change the directives User= and Group= to the desired user. 
You might notice that the EnvironmentFile= and ExecStart= directives are changed. 
The EnvironmentFile= directive specifies the path to a file with environment 
variables that can be used by the service. Here you can define variables like 
SCHEDULER_RUNS, AIRFLOW_HOME or AIRFLOW_CONFIG. Ma


sudo apt-get install postgresql postgresql-contrib libpq-dev python3-psycopg2

pip install apache-airflow[postgres]
pip install psycopg2

airflow.cfg
change executor to 
executor = LocalExecutor
# sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@localhost/airflow
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@localhost:5432/airflow

# maybe change
parallelism = 2
dag_concurrency = 2
max_active_runs_per_dag = 2
#load_default_connections = True
load_default_connections = False

# create the airflow tables in postgres
sudo -u postgres psql
CREATE USER airflow WITH PASSWORD 'airflow';
CREATE DATABASE airflow;
# GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO airflow;
# GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO airflow;
GRANT ALL PRIVILEGES ON DATABASE airflow to airflow;

# and edit the postgres config files 
sudo vi /etc/postgresql/11/main/pg_hba.conf
didn't change anything
change this 
# IPv4 local connections:
host    all             all             127.0.0.1/32         md5
to this
# IPv4 local connections:
host    all             all             0.0.0.0/0            trust
sudo vi /etc/postgresql/11/main/postgresql.conf
and change 
listen_addresses = 'localhost'
to
listen_addresses = '*'
# restart the pg service 
sudo service postgresql restart

# initialize the airflow db 
airflow initdb 
# copy over the airflow service files 

sudo cp setup/airflow-*.service /etc/systemd/system/.



sudo systemctl enable airflow-webserver
sudo systemctl start airflow-webserver
sudo systemctl enable airflow-scheduler
sudo systemctl start airflow-scheduler
sudo systemctl status airflow-webserver
sudo systemctl status airflow-scheduler

sudo journalctl -u airflow-webserver.service -f
sudo journalctl -u airflow-webserver.service -u airflow-scheduler.service -n 10
sudo journalctl -u airflow-webserver.service -n 10
sudo journalctl -u airflow-scheduler.service -n 10


sudo systemctl daemon-reload
sudo systemctl restart airflow-webserver
sudo systemctl restart airflow-scheduler
sudo systemctl stop 
sudo systemctl disable 

chmod 664 /etc/systemd/system/airflow-scheduler.service

sudo systemctl start airflow-webserver.service
sudo systemctl start airflow-scheduler.service
sudo systemctl status airflow-webserver.service
sudo systemctl status airflow-scheduler.service
sudo systemctl enable airflow-webserver.service
sudo systemctl enable airflow-scheduler.service

# airflow 
######################################



######################################
# docker 
sudo yum install -y docker
sudo service docker start
docker info 
docker run -d -p 80:8080 chandupriya/nodedemo:0.1
sudo service docker start
sudo usermod -a -G docker ec2-user

touch Dockerfile
FROM ubuntu:18.04

# Install dependencies
RUN apt-get update && \
 apt-get -y install apache2

# Install apache and write hello world message
RUN echo 'Hello World!' > /var/www/html/index.html

# Configure apache
RUN echo '. /etc/apache2/envvars' > /root/run_apache.sh && \
 echo 'mkdir -p /var/run/apache2' >> /root/run_apache.sh && \
 echo 'mkdir -p /var/lock/apache2' >> /root/run_apache.sh && \ 
 echo '/usr/sbin/apache2 -D FOREGROUND' >> /root/run_apache.sh && \ 
 chmod 755 /root/run_apache.sh

EXPOSE 80

CMD /root/run_apache.sh

docker build -t hello-world .
docker run -t -i -p 80:80 hello-world

# run and enter the container
docker run -ti ubuntu
exit 

# show docker processes 
docker ps
docker ps -a 

# restart a container 
docker start -ai 11cc47339ee1

# docker 
######################################




pip install flask_pymongo flask-cors

sudo systemctl start <service-name>
$ sudo systemctl stop <service-name>
$ sudo systemctl restart <service-name>
$ sudo systemctl status <service-name>

/etc/systemd/system/microblog.service
[Unit]
Description=Microblog web application
After=network.target

[Service]
User=ubuntu
WorkingDirectory=/home/ubuntu/microblog
ExecStart=/home/ubuntu/microblog/venv/bin/gunicorn -b localhost:8000 -w 4 microblog:app
Restart=always

[Install]
WantedBy=multi-user.target

# to move to setup 
######################################



# did not work
psycopg2 
# works on ubuntu, does not work on chromebook 
conda install -c conda-forge eccodes cfgrib
# works on chromebook 
pip install cfgrib

conda update conda 
conda config --add channels conda-forge
# does not work
conda install -c conda-forge psycopg2 numpy pandas dask netCDF4 xarray spyder matplotlib
# works 
conda install -c conda-forge psycopg2 numpy pandas dask xarray matplotlib
# works on ubuntu, does not work on chromebook 
conda install -c conda-forge eccodes cfgrib
# works on chromebook 
sudo apt-get install libeccodes0
pip install cfgrib
python -m cfgrib selfcheck
cd pgrad
# pip install -r requirements.txt

conda create -n cartopy -c conda-forge cartopy netCDF4 xarray 
conda activate cartopy
conda install spyder‑kernels


create test files on s3://to_process
temp1.txt
temp2.txt
etc

import boto3

s3 = boto3.resource('s3')

def upload_file_to_S3(filename, key, bucket_name):
    s3.Bucket(bucket_name).upload_file(filename, key)

# General ssh and scp commands 

# ssh
ssh -i ~/.ssh/sundownerwatch-IAM-keypair.pem ubuntu@ec2-54-202-214-49.us-west-2.compute.amazonaws.com

# scp
scp -i ~/.ssh/sundownerwatch-IAM-keypair.pem src/* ubuntu@ec2-54-202-214-49.us-west-2.compute.amazonaws.com:/home/ubuntu/raceCast/src/.


# set up a keypair from local to pg
ssh -i ~/.ssh/sundownerwatch-IAM-keypair.pem ubuntu@ec2-34-222-54-126.us-west-2.compute.amazonaws.com


```
Sid not install python-dev but may be needed
sudo apt-get install python3-dev

sudo apt-get install pip
pip3 install psycopg2
pip3 install psycopg2-binary

# create conda environment for pg
conda create -n pg_env
conda activate pg_env
conda install -c conda-forge psycopg2 numpy pandas
pip install spyder‑kernels

sudo apt-get install libpq-dev
# did not install python-dev but may be needed
sudo apt-get install python3-dev
sudo apt-get install python3-psycopg2

sudo apt-get install pip
pip3 install psycopg2
# dont know if this is needed or not
pip3 install psycopg2-binary

# create conda environment for pg
conda create -n pg_env
conda activate pg_env
conda install -c conda-forge psycopg2 numpy pandas
pip install spyder‑kernels

conda install -c conda-forge s3fs

sudo apt-get install s3fs

dash environment 

conda create -n dash_env
conda activate dash_env
conda install -c conda-forge psycopg2 numpy pandas dash falcon
pip install spyder‑kernels

  
df = pd.read_csv('s3://gps-data-processed/gps_stream_3.csv')
fs = s3fs.S3FileSystem(anon=False)
fs = s3fs.S3FileSystem(anon=True)
fs.ls('gps-data-processed')
fs.touch('gps-data-processed/test.txt') 
fs.put(file_path,s3_bucket + "/" + rel_path)
fs = s3fs.S3FileSystem(anon=False, key='<Access Key>', secret='<Secret Key>')


